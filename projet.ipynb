{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "data_list = []\n",
    "# The path containing my datas\n",
    "path_data = './data'\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)\n",
    "# The name of the files\n",
    "usagers_2022_file = 'usagers_2022.csv'\n",
    "usagers_2022_link = 'https://www.data.gouv.fr/fr/datasets/r/62c20524-d442-46f5-bfd8-982c59763ec8'\n",
    "vehicules_2022_file = 'vehicules_2022.csv'\n",
    "vehicules_2022_link = 'https://www.data.gouv.fr/fr/datasets/r/c9742921-4427-41e5-81bc-f13af8bc31a0'\n",
    "lieux_2022_file = 'lieux_2022.csv'\n",
    "lieux_2022_link = 'https://www.data.gouv.fr/fr/datasets/r/a6ef711a-1f03-44cb-921a-0ce8ec975995'\n",
    "caract_2022_file = 'caract_2022.csv'\n",
    "caract_2022_link = 'https://www.data.gouv.fr/fr/datasets/r/5fc299c0-4598-4c29-b74c-6a67b0cc27e7'\n",
    "\n",
    "data_list.append((usagers_2022_file,usagers_2022_link))\n",
    "data_list.append((vehicules_2022_file,vehicules_2022_link))\n",
    "data_list.append((lieux_2022_file,lieux_2022_link))\n",
    "data_list.append((caract_2022_file,caract_2022_link))\n",
    "\n",
    "usagers_2021_file = 'usagers_2021.csv'\n",
    "usagers_2021_link = 'https://www.data.gouv.fr/fr/datasets/r/ba5a1956-7e82-41b7-a602-89d7dd484d7a'\n",
    "\n",
    "vehicules_2021_file = 'vehicules_2021.csv'\n",
    "vehicules_2021_link = 'https://www.data.gouv.fr/fr/datasets/r/0bb5953a-25d8-46f8-8c25-b5c2f5ba905e'\n",
    "\n",
    "lieux_2021_file = 'lieux_2021.csv'\n",
    "lieux_2021_link = 'https://www.data.gouv.fr/fr/datasets/r/8a4935aa-38cd-43af-bf10-0209d6d17434'\n",
    "\n",
    "caract_2021_file = 'caract_2021.csv'\n",
    "caract_2021_link = 'https://www.data.gouv.fr/fr/datasets/r/85cfdc0c-23e4-4674-9bcd-79a970d7269b'\n",
    "\n",
    "data_list.append((usagers_2021_file,usagers_2021_link))\n",
    "data_list.append((vehicules_2021_file,vehicules_2021_link))\n",
    "data_list.append((lieux_2021_file,lieux_2021_link))\n",
    "data_list.append((caract_2021_file,caract_2021_link))\n",
    "\n",
    "for filename,link in data_list:\n",
    "    if os.path.exists(os.path.join(path_data, filename)):\n",
    "        print('The file %s already exists.' % os.path.join(path_data, filename))\n",
    "    else:\n",
    "        r = requests.get(link)\n",
    "        with open(os.path.join(path_data, filename), 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(f'Downloaded file {os.path.join(path_data, filename)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading library and spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.ops_on_diff_frames', True)   \n",
    "# Set the compute.max_rows option to a larger value\n",
    "ps.config.set_option('compute.max_rows', None)  \n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "conf = SparkConf().setAppName(\"Data cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning usager 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_data, usagers_2021_file)\n",
    "sdf = ps.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sdf.loc[0, 'id_usager'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sdf.loc[0, 'id_vehicule'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see some string type for id_usager and id_vehicule that should be interger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['sexe'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['place'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['grav'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following columns : sexe, place and grav should not have -1 as values so we can think of seeing how many bad values we have and decide if we should delete the lines with thoses values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf['sexe'] == -1).sum() ,(sdf['place'] == -1).sum(),(sdf['grav'] == -1).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's delete the rows with bad values and check again and we drop the useless column num_veh because xe have id_vehicule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_incorrect_values_usager(sdf):\n",
    "    sdf = sdf.loc[(sdf['sexe'] != -1) & (sdf['place'] != -1) & (sdf['grav'] != -1)]\n",
    "    sdf = sdf.drop(columns=['num_veh'])\n",
    "    return sdf\n",
    "sdf = delete_incorrect_values_usager(sdf)\n",
    "# sdf = sdf.loc[(sdf['sexe'] != -1) & (sdf['place'] != -1) & (sdf['grav'] != -1)]\n",
    "(sdf['sexe'] == -1).sum() ,(sdf['place'] == -1).sum(),(sdf['grav'] == -1).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to attack column with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = sdf.isnull().sum()\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column an_nais has 3067 missing values so we can also delete 3067 rows because we will need the column to show some datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.dropna()\n",
    "spam = sdf.isnull().sum()\n",
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets checkout categorical variables and all types\n",
    "### Columns that should be categoricals:\n",
    "- place\n",
    "- catu\n",
    "- grav\n",
    "- sexe\n",
    "- trajet\n",
    "- secu1\n",
    "- secu2\n",
    "- secu3\n",
    "- locp\n",
    "- actp\n",
    "- etatp\n",
    "### Other types\n",
    "- id_usager, id_vehicules, Num_Acc and num_veh are becoming string. We tried to spilt the space of every row in the id_usager and id_vehicule column but apprently those type of operations are impossible with pands on spark. It's not allowed\n",
    "- column should be an Integer\n",
    "- we are adding a column annee for the year of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['annee'] = 2021\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usager_dtypes = {\n",
    "    'Num_Acc': str,\n",
    "    'id_usager': str,\n",
    "    'id_vehicule': str,\n",
    "    'place': np.int32,\n",
    "    'catu': np.int8,\n",
    "    'grav': np.int16,\n",
    "    'sexe': np.int8,\n",
    "    'an_nais': np.int64,\n",
    "    'trajet': np.int32,\n",
    "    'secu1': np.int32,\n",
    "    'secu2': np.int32,\n",
    "    'secu3': np.int32,\n",
    "    'locp': np.int32,\n",
    "    'actp': np.int32,\n",
    "    'etatp': np.int32,\n",
    "    'annee': np.int64,\n",
    "}\n",
    "sdf = sdf.astype(usager_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_usager_df(sdf,year):\n",
    "    sdf = delete_incorrect_values_usager(sdf)\n",
    "    sdf = sdf.dropna()\n",
    "    sdf['annee'] = year\n",
    "    sdf = sdf.astype(usager_dtypes)\n",
    "    return sdf\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's clean usager 2022 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = os.path.join(path_data, usagers_2022_file)\n",
    "sdf2 = ps.read_csv(file_path2, sep=';')\n",
    "sdf2 = clean_usager_df(sdf2,2022)\n",
    "# sdf2 = sdf2.loc[(sdf2['sexe'] != -1) & (sdf2['place'] != -1) & (sdf2['grav'] != -1)]\n",
    "# sdf2 = sdf2.dropna()\n",
    "# sdf2['annee'] = 2022\n",
    "# sdf2 = sdf2.astype(usager_dtypes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's concat the datas and save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usager_df = ps.concat([sdf, sdf2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usager_df = usager_df.astype(usager_dtypes)\n",
    "usager_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usager_df.to_parquet(os.path.join(\"usager_cleaned.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning 2021 vehicules csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path3 = os.path.join(path_data, vehicules_2021_file)\n",
    "sdf3 = ps.read_csv(file_path3, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns occutc will disappear cause it has a lot of null values, we also dont need the column num_veh because we have id_vehicule already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3['catv'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catv column should not have -1 values so we delete the rows with that value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_incorrect_values_vehicules(sdf):\n",
    "    sdf = sdf.loc[(sdf['catv'] != -1)]\n",
    "    sdf = sdf.drop(columns=['occutc'])\n",
    "    sdf = sdf.drop(columns=['num_veh'])\n",
    "    return sdf\n",
    "sdf3 = delete_incorrect_values_vehicules(sdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf3['catv'] == -1).sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = sdf3.isnull().sum()\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addind the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3['annee'] = 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicule_dtypes = {\n",
    "    'Num_Acc': str,\n",
    "    'id_vehicule': str,\n",
    "    'senc': np.int32,\n",
    "    'catv': np.int32,\n",
    "    'obs': np.int32,\n",
    "    'obsm': np.int32,\n",
    "    'choc': np.int32,\n",
    "    'manv': np.int32,\n",
    "    'motor': np.int32,\n",
    "    'annee': np.int64,\n",
    "    \n",
    "}\n",
    "sdf3 = sdf3.astype(vehicule_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vehicule_df(sdf,year):\n",
    "    sdf = delete_incorrect_values_vehicules(sdf)\n",
    "    sdf['annee'] = year\n",
    "    sdf = sdf.astype(vehicule_dtypes)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's clean 2022 vehicules data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path4 = os.path.join(path_data, vehicules_2022_file)\n",
    "sdf4 = ps.read_csv(file_path4, sep=';')\n",
    "sdf4 = clean_vehicule_df(sdf4,2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's concat the datas and save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicule_df = ps.concat([sdf3, sdf4])\n",
    "vehicule_df = vehicule_df.astype(vehicule_dtypes)\n",
    "# vehicule_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicule_df.to_parquet(os.path.join(\"vehicule_cleaned.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning: caract_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bilan caract_2021:\n",
    "- type des colonnes \n",
    "- mise en date time de an \n",
    "- drop de la colonne commune (com)\n",
    "- mise en float de lat et long\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_data, caract_2021_file)\n",
    "sdf_cara_21 = ps.read_csv(file_path, sep=';')\n",
    "sdf_cara_21.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out for null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21['lat'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we look for distinct values for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21['dep'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21['lat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf_cara_21['int']==-1).sum()  # count the number of -1 in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all values for the columns are ok , they all have distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_21.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#columns data types \n",
    "\n",
    "** categotical columns :\n",
    "- lum\n",
    "- agg\n",
    "- int\n",
    "- atm\n",
    "- col\n",
    "- dep\n",
    "- com\n",
    "- lat\n",
    "- long\n",
    "- \n",
    "**numerical columns :\n",
    "- jour\n",
    "- mois\n",
    "\n",
    "  \n",
    "** continuos columns to be decided later:\n",
    "- adr \n",
    "- Num_Acc\n",
    "- lat\n",
    "- long\n",
    "  \n",
    "** date columns:\n",
    "- hrmn\n",
    "- an\n",
    "\n",
    "\n",
    "lets build the dictionary of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caract_types ={\n",
    "    'lum': np.int16,\n",
    "    'agg': np.int16,\n",
    "    'int': np.int16,\n",
    "    'atm': np.int16,\n",
    "    'col': np.int16,\n",
    "    'dep': np.int16,\n",
    "    'Num_Acc': str,\n",
    "    'jour': np.int64,\n",
    "    'mois': np.int64,\n",
    "    'adr' : str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = os.path.join(path_data, caract_2021_file)\n",
    "cleaned_caract_21 = sdf_cara_21.astype(caract_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_caract_21.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_caract_21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_caract_21['lat'] = cleaned_caract_21['lat'].astype(float)\n",
    "cleaned_caract_21['long'] = cleaned_caract_21['long'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_caract_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns com\n",
    "cleaned_caract_21 = cleaned_caract_21.drop(columns=['com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_caract_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_caract_21['an'] = ps.to_datetime(cleaned_caract_21['an'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a dataframe and a year as input and returns a cleaned dataframe\n",
    "def clean_caract_df(sdf,year):\n",
    "    #type conversion\n",
    "    sdf = sdf.astype(caract_types)\n",
    "    \n",
    "    #drop the columns com\n",
    "    sdf = sdf.drop(columns=['com'])\n",
    "    \n",
    "    # replace ',' by '.' and convert to float\n",
    "    sdf['lat'] = sdf['lat'].astype(float)\n",
    "    sdf['long'] = sdf['long'].astype(float)\n",
    "    \n",
    "    # convert the column 'an' to datetime\n",
    "    # sdf['an'] = ps.to_datetime(sdf['an'], format='%Y')\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the cleaned data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_caract_21.to_parquet('caract_2021.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning : du fichier caract_2022.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_data, caract_2022_file)\n",
    "sdf_cara_22 = ps.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_cara_22.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the column name AccidentId to Num_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sdf_cara_22 = sdf_cara_22.rename(columns={'Accident_Id': 'Num_Acc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_cara_22 = clean_caract_df(sdf_cara_22,2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_cara_22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_cara_22.to_parquet('caract_2022.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concanate the two dataframes caract_2021 and caract_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carateristiques = ps.concat([cleaned_caract_21, cleaned_cara_22])\n",
    "carateristiques.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_carart = carateristiques.astype(caract_types)\n",
    "clean_carart.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_carart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_carart.to_parquet('caracteristiques.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_carart.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_carart.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning du fichier lieu_2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_data, lieux_2021_file)\n",
    "\n",
    "sdf_l_21 = ps.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bilan:\n",
    "- type des colonnes\n",
    "- drop column V2 , lartpc and larrout ,pr et pr1\n",
    "- drop les na \n",
    "- ajout de la colonne annee\n",
    "- format de la colonne annee en date time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21['larrout'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21['v2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21['lartpc'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop les colonnes \n",
    "- V2\n",
    "- lartpc\n",
    "- larrout  \n",
    "- pr  \n",
    "- pr1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21 = sdf_l_21.drop(columns=['lartpc'])\n",
    "sdf_l_21 = sdf_l_21.drop(columns=['v2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21 = sdf_l_21.drop(columns=['larrout'])\n",
    "sdf_l_21 = sdf_l_21.drop(columns=['pr'])\n",
    "sdf_l_21 = sdf_l_21.drop(columns=['pr1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_21.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type pour les fichies lieu_2021 et lieu_2022\n",
    "\n",
    "** categotical columns :\n",
    "- catr\n",
    "- circ\n",
    "- vosp\n",
    "- prof\n",
    "- plan\n",
    "- surf\n",
    "- infra\n",
    "- situ\n",
    "  \n",
    "  \n",
    "\n",
    "**numerical columns :\n",
    "- nbv\n",
    "- pr1\n",
    "- larrout\n",
    "- vma\n",
    "\n",
    "** continuos columns:\n",
    "- Num_Acc\n",
    "- v1\n",
    "- pr\n",
    "- voie\n",
    "\n",
    "**deleted columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu_types ={\n",
    "    'Num_Acc': str,\n",
    "    'catr': np.int64,\n",
    "    'voie': str,\n",
    "    'v1': str,\n",
    "    'circ': np.int64,\n",
    "    'nbv': np.int64,\n",
    "    'vosp': np.int64,\n",
    "    'prof': np.int64,\n",
    "    'plan': np.int64,\n",
    "    'surf': np.int64,\n",
    "    'infra': np.int64,\n",
    "    'situ': np.int64,\n",
    "    'vma': np.int64 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21 = sdf_l_21.astype(lieu_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21 = clean_lieux_21.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21['annee'] = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_lieux_21['annee'] = ps.to_datetime(clean_lieux_21['annee'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_lieux_21.to_parquet('lieux_2021.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la fonction de clean pour les lieu\n",
    "def clean_lieux_df(sdf, year):\n",
    "    # drop les column lartpc et v2 , larrout , pr , pr1\n",
    "    sdf = sdf.drop(columns=['lartpc'])\n",
    "    sdf = sdf.drop(columns=['v2'])\n",
    "    sdf = sdf.drop(columns=['larrout'])\n",
    "    sdf = sdf.drop(columns=['pr'])\n",
    "    sdf = sdf.drop(columns=['pr1'])\n",
    "    \n",
    "    #drop the rows with missing values\n",
    "    sdf = sdf.dropna()\n",
    "    \n",
    "    # covert the type \n",
    "    sdf = sdf.astype(lieu_types)\n",
    "    \n",
    "    \n",
    "    # ajout de l'année\n",
    "    sdf['annee'] = year\n",
    "    \n",
    "    # ajout de type annee\n",
    "    # sdf['annee'] = ps.to_datetime(sdf['annee'], format='%Y')\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning du fichier lieu2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_data, lieux_2022_file)\n",
    "sdf_l_22 = ps.read_csv(file_path, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_22.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_l_22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_22 = clean_lieux_df(sdf_l_22,2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lieux_22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_lieux_22.to_parquet('lieux_2022.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joining the file lieux 2021 and 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu = ps.concat([clean_lieux_21,clean_lieux_22])\n",
    "\n",
    "lieu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lieu.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu = lieu.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lieu = lieu.astype(lieu_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieu.to_parquet('lieu.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading files from parquet format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = ps.read_parquet(os.path.join(\"vehicule_cleaned.parquet\"),dtype=vehicule_dtypes)\n",
    "df_v =df_v.astype(vehicule_dtypes)\n",
    "\n",
    "df_u = ps.read_parquet(os.path.join(\"usager_cleaned.parquet\"),dtype=usager_dtypes)\n",
    "df_u =df_u.astype(usager_dtypes)\n",
    "\n",
    "df_carac = ps.read_parquet(os.path.join(\"caracteristiques.parquet\"))\n",
    "df_carac =df_carac.astype(caract_types)\n",
    "\n",
    "df_lieu = ps.read_parquet(os.path.join(\"lieu.parquet\"))\n",
    "df_lieu =df_lieu.astype(lieu_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the column age for some visualisation after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u[\"age\"] = df_u[\"annee\"] - df_u[\"an_nais\"]\n",
    "age_col = df_u[\"age\"]\n",
    "age_col.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's caluculate Interquartile difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = age_col.quantile(0.25)\n",
    "q3 = age_col.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "print(iqr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = age_col.skew()\n",
    "kurtosis = age_col.kurtosis()\n",
    "\n",
    "print(\"\\nAsymétrie (skewness) :\")\n",
    "print(skewness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAplatissement (kurtosis) :\")\n",
    "print(kurtosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = px.box(age_col.to_pandas(), y=\"age\", title=f\"Boxplot de l'age \")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the distribution of age and all the quantille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question : Repartition of accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the repartion of accidents per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondance_mois = {\n",
    "    1: 'Janvier',\n",
    "    2: 'Février',\n",
    "    3: 'Mars',\n",
    "    4: 'Avril',\n",
    "    5: 'Mai',\n",
    "    6: 'Juin',\n",
    "    7: 'Juillet',\n",
    "    8: 'Août',\n",
    "    9: 'Septembre',\n",
    "    10: 'Octobre',\n",
    "    11: 'Novembre',\n",
    "    12: 'Décembre'\n",
    "}\n",
    "df_carac['nom_mois'] = df_carac['mois'].map(correspondance_mois)\n",
    "fig = px.histogram(df_carac, x='nom_mois', \n",
    "title='Répartition des accidents par mois de l\\'année',\n",
    "labels={'nom_mois': 'Mois de l\\'année', 'count': 'Nombre d\\'accidents'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usager Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_usager_carac = ps.merge(df_u, df_carac, on =[\"Num_Acc\"])\n",
    "merged_usager_carac = merged_usager_carac.astype({**usager_dtypes,**caract_types})\n",
    "#merged_usager_carac.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_usager_lieu = ps.merge(df_u, df_lieu, on =[\"Num_Acc\"])\n",
    "#merged_usager_lieu = merged_usager_lieu.astype({**lieu_types,**usager_dtypes})\n",
    "# merged_usager_lieu.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_usager_veh = ps.merge(df_u, df_v, on =[\"Num_Acc\",\"id_vehicule\",\"annee\"])\n",
    "merged_usager_veh = merged_usager_veh.astype({**vehicule_dtypes,**usager_dtypes})\n",
    "# merged_usager_veh.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show  \" Distribution de la gravité des accidents par catégorie de route \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catr_names = {\n",
    "    1: 'Autoroute',\n",
    "    2: 'Route nationale',\n",
    "    3: 'Route départementale',\n",
    "    4: 'Voie communale',\n",
    "    5: 'Hors réseau public',\n",
    "    6: 'Parc de stationnement ouvert à la circulation publique',\n",
    "    7: 'Routes de métropole urbaine',\n",
    "    9: 'Autre'\n",
    "}\n",
    "\n",
    "grav_names = {\n",
    "    1: 'Indemne',\n",
    "    2: 'Tué',\n",
    "    3: 'Blessé hospitalisé',\n",
    "    4: 'Blessé léger'\n",
    "}\n",
    "merged_usager_lieu['grav_name'] = merged_usager_lieu['grav'].map(grav_names)\n",
    "merged_usager_lieu['catr_name'] = merged_usager_lieu['catr'].map(catr_names)\n",
    "grouped_data = merged_usager_lieu.groupby(['catr_name', 'grav_name']).size().reset_index(name='count')\n",
    "fig_1 = px.bar(grouped_data, x='catr_name', y='count', color='grav_name',\n",
    "title='Distribution de la gravité des blessures d\\'accidents par catégorie de route',\n",
    "labels={'catr_name': 'Profil de la route', 'count': 'Nombre d\\'usagers', 'grav_name': 'Gravité Blessure'},\n",
    "barmode='group')\n",
    "\n",
    "fig_1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are actually less deadly accidents on \" Autoroute \" roads than people might think. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catv_names = {\n",
    "    # 0: \"Indéterminable\",\n",
    "    1: \"Bicyclette\",\n",
    "    2: \"Cyclomoteur <50cm3\",\n",
    "    # 3: \"Voiturette (Quadricycle à moteur carrossé)\",\n",
    "    # 4: \"Référence inutilisée depuis 2006 (scooter immatriculé)\",\n",
    "    # 5: \"Référence inutilisée depuis 2006 (motocyclette)\",\n",
    "    # 6: \"Référence inutilisée depuis 2006 (side-car)\",\n",
    "    7: \"VL seul\",\n",
    "    # 8: \"Référence inutilisée depuis 2006 (VL + caravane)\",\n",
    "    # 9: \"Référence inutilisée depuis 2006 (VL + remorque)\",\n",
    "    # 10: \"VU seul 1,5T <= PTAC <= 3,5T avec ou sans remorque\",\n",
    "    # 11: \"Référence inutilisée depuis 2006 (VU (10) + caravane)\",\n",
    "    # 12: \"Référence inutilisée depuis 2006 (VU (10) + remorque)\",\n",
    "    \n",
    "    # 13: \"PL seul 3,5T <PTCA <= 7,5T\",\n",
    "    # 14: \"PL seul > 7,5T\",\n",
    "    # 15: \"PL > 3,5T + remorque\",\n",
    "    \n",
    "    # 16: \"Tracteur routier seul\",\n",
    "    # 17: \"Tracteur routier + semi-remorque\",\n",
    "    # 18: \"Référence inutilisée depuis 2006 (transport en commun)\",\n",
    "    # 19: \"Référence inutilisée depuis 2006 (tramway)\",\n",
    "    # 20: \"Engin spécial\",\n",
    "    # 21: \"Tracteur agricole\",\n",
    "    30: \"Scooter < 50 cm3\",\n",
    "    31: \"Motocyclette > 50 cm3 et <= 125 cm3\",\n",
    "    32: \"Scooter > 50 cm3 et <= 125 cm3\",\n",
    "    33: \"Motocyclette > 125 cm3\",\n",
    "    34: \"Scooter > 125 cm3\",\n",
    "    # 35: \"Quad léger <= 50 cm3 (Quadricycle à moteur non carrossé)\",\n",
    "    # 36: \"Quad lourd > 50 cm3 (Quadricycle à moteur non carrossé)\",\n",
    "    37: \"Autobus\",\n",
    "    # 38: \"Autocar\",\n",
    "    # 39: \"Train\",\n",
    "    # 40: \"Tramway\",\n",
    "    # 41: \"3RM <= 50 cm3\",\n",
    "    # 42: \"3RM > 50 cm3 <= 125 cm3\",\n",
    "    # 43: \"3RM >\"\n",
    "}\n",
    "\n",
    "merged_usager_veh['catv_name'] = merged_usager_veh['catv'].map(catv_names)\n",
    "merged_usager_veh['grav_name'] = merged_usager_veh['grav'].map(grav_names)\n",
    "\n",
    "grouped_data = merged_usager_veh.groupby(['catv_name', 'grav_name']).size().reset_index(name='count')\n",
    "\n",
    "fig_2 = px.bar(grouped_data, x='catv_name', y='count', color='grav_name',\n",
    "title='Distribution de la gravité des blessures d\\'accidents par rapport au vehicule impliqué',\n",
    "labels={'catv_name': 'Catégorie de vehicule', 'count': 'Nombre d\\'usagers', 'grav_name': 'Gravité Blessure'},\n",
    "barmode='group')\n",
    "\n",
    "fig_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \" Vl seul\" actually has more accidents than all the other types of vehicules combined. That type of vehicule might be the problem, who knows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you guessed, man are the most involved in accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_names = {\n",
    "    # -1: 'Non Renseigné',\n",
    "    1: 'Plat',\n",
    "    2: 'Pente',\n",
    "    3: 'Sommet de côte',\n",
    "    4: 'Bas de côte'\n",
    "}\n",
    "sexe_names = {\n",
    "    1: 'Masculin',\n",
    "    2: 'Feminin'\n",
    "}\n",
    "\n",
    "merged_usager_lieu['prof_name'] = merged_usager_lieu['prof'].map(prof_names)\n",
    "merged_usager_lieu['sexe_name'] = merged_usager_lieu['sexe'].map(sexe_names)\n",
    "\n",
    "grouped_data = merged_usager_lieu.groupby(['prof_name', 'sexe_name']).size().reset_index(name='count')\n",
    "\n",
    "fig_3 = px.bar(grouped_data, x='prof_name', y='count', color='sexe_name',\n",
    "title='Distribution des sexe par type du lieu d\\'accident',\n",
    "labels={'prof_name': 'Type de la route', 'count': 'Nombre d\\'usagers', 'sexe_name': 'Sexe'},\n",
    "barmode='group')\n",
    "\n",
    "fig_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you guessed, men are more involved in accidents than women on any type of roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catu_names = {\n",
    "    1: 'Conducteur',\n",
    "    2: 'Passager',\n",
    "    3: 'Piéton'\n",
    "}\n",
    "# catu_values = merged_usager_lieu['catu'].cat.categories\n",
    "# mapped_values = [catu_names[value] for value in catu_values]\n",
    "# mapping = dict(zip(catu_values, mapped_values))\n",
    "# merged_usager_lieu['catu_name'] = merged_usager_lieu['catu'].map(mapping)\n",
    "merged_usager_lieu['catu_name'] = merged_usager_lieu['catu'].map(catu_names)\n",
    "grouped_data = merged_usager_lieu.groupby(['catu_name', 'grav_name']).size().reset_index(name='count')\n",
    "\n",
    "fig_4 = px.bar(grouped_data, x='catu_name', y='count', color='grav_name',\n",
    "title='Distribution des usagers par rapport à la gravité de leur accidents',\n",
    "labels={'catu_name': 'Catégorie d\\'usager', 'count': 'Nombre d\\'usagers', 'grav_name': 'Gravité Accident'},\n",
    "barmode='group')\n",
    "\n",
    "# Afficher le graphique\n",
    "fig_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question : \" Usage des types composites \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the data that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_veh_lieu = ps.merge(df_v, df_lieu, on =[\"Num_Acc\"])\n",
    "merged_veh_lieu = merged_veh_lieu.astype({**vehicule_dtypes,**lieu_types})\n",
    "merged_veh_lieu['categorie_route'] = merged_veh_lieu['catr'].map(catr_names)\n",
    "\n",
    "# merged_veh_lieu.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the columns that we need and group it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "df = merged_veh_lieu[[\"Num_Acc\",\"categorie_route\",\"id_vehicule\"]]\n",
    "df = df.to_spark()\n",
    "grouped_df = df.groupby(\"Num_Acc\", \"categorie_route\") \\\n",
    "    .agg(collect_list(struct(\"id_vehicule\")).alias(\"id_vehicule_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is exactely what we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats resume with plotly for carateristique: \n",
    "- describe \n",
    "- skewness of each column\n",
    "- kurtosis of each column\n",
    "- box plot of each column\n",
    "- column correlation\n",
    "- histogram\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statitique descriptive de chaque colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carac.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our remarks are:\n",
    "- most accident occured during the day\n",
    "- most accident occured in agglomerations (cities, towns etc)\n",
    "- most accident occured during normal atmospheric conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skewness and kurtosis of each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------skewness------------------------\")\n",
    "skewness_caract = df_carac.skew()\n",
    "print(skewness_caract)\n",
    "print(\"--------------kurtosis------------------------\")\n",
    "kurto_caract = df_carac.kurtosis()\n",
    "print(kurto_caract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifions avec des histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let see the box plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  the box of accident that occured during the day through the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lumiere : conditions de luminosité de l'accident\n",
    "lumiere = { \n",
    "    1: 'Plein jour',\n",
    "    2: 'Crépuscule ou aube',\n",
    "    3: 'Nuit sans éclairage public',\n",
    "    4: 'Nuit avec éclairage public non allumé',\n",
    "    5: 'Nuit avec éclairage public allumé'\n",
    "}\n",
    "df_carac['periode_lum'] = df_carac['lum'].map(lumiere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atm : conditions atmosphériques\n",
    "atmosphere = {\n",
    "    1: 'Normale',\n",
    "    2: 'Pluie légère',\n",
    "    3: 'Pluie forte',\n",
    "    4: 'Neige - grêle',\n",
    "    5: 'Brouillard - fumée',\n",
    "    6: 'Vent fort - tempête',\n",
    "    7: 'Temps éblouissant',\n",
    "    8: 'Temps couvert',\n",
    "    9: 'Autre'\n",
    "}\n",
    "df_carac['conditions_atmos'] = df_carac['atm'].map(atmosphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg : agglomération\n",
    "agglomeration = {\n",
    "    1: 'Hors agglomération',\n",
    "    2: 'En agglomération'\n",
    "}\n",
    "df_carac['agglomeration'] = df_carac['agg'].map(agglomeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_carac, x='an', y='Num_Acc')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_carac, x='an', y='periode_lum', color='an', title='repartition de la periode des accidents par année')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pas tres convaincant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_carac, x='periode_lum', \n",
    "title='Répartition des accidents par intensité durant  l\\'année',\n",
    "labels={'periode_lum': 'l\\'année', 'count': 'Nombre d\\'accidents'}, color='an')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_carac, x='conditions_atmos',\n",
    "title='Répartition des accidents par conditions atmosphériques',\n",
    "labels={'conditions_atmos': 'Conditions atmosphériques', 'count': 'Nombre d\\'accidents'}, color='conditions_atmos')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_carac, x='agglomeration',\n",
    "title='Répartition des accidents par agglomération(ville ,citée, village, etc...)',\n",
    "labels={'agglomeration': 'Agglomération', 'count': 'Nombre d\\'accidents'}, color='agglomeration')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les repartion des accidents par mois, an "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_carac, x=\"an\", nbins=2, title=\"Répartition des accidents par année\", color=\"an\" , labels={'an': 'Année', 'count': 'Nombre d\\'accidents'})\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df_carac, x=\"nom_mois\", nbins=12, title=\"Répartition des accidents par mois\", color=\"mois\" , labels={'nom_mois': 'Mois de l\\'année', 'count': 'Nombre d\\'accidents'})\n",
    "fig.show()\n",
    "\n",
    "# pas tres pertinent ces 2\n",
    "fig = px.histogram(df_carac, x=\"jour\", nbins=31, title=\"Répartition des accidents par jour\", color=\"jour\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.pie(df_carac, values=\"Num_Acc\", names=\"jour\", title=\"Répartition des accidents par jour\", color=\"jour\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stat resume with plotly for lieux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieu.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en moyenne les accidents sont en vitesse maximale 58 km/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regardons les histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#situation : situation de l'accident\n",
    "situation = {\n",
    "    1: 'Sur chaussée',\n",
    "    2: 'Sur bande d\\'arrêt d\\'urgence',\n",
    "    3: 'Sur accotement',\n",
    "    4: 'Sur trottoir',\n",
    "    5: 'Sur piste cyclable',\n",
    "    6: 'Sur autre voie spéciale',\n",
    "    9: 'Autre'\n",
    "}\n",
    "df_lieu['situation_accident'] = df_lieu['situ'].map(situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_lieu, x='situation_accident',\n",
    "title='Répartition des accidents par situation de l\\'accident',\n",
    "labels={'situation_accident': 'Situation de l\\'accident', 'count': 'Nombre d\\'accidents'}, color='situation_accident')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
